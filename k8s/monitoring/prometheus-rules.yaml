# Prometheus Alerting Rules for Shahin GRC Platform HA
# Requires Prometheus Operator installed in cluster
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: shahin-grc-alerts
  namespace: shahin-grc
  labels:
    app.kubernetes.io/name: shahin-grc
    prometheus: kube-prometheus
    role: alert-rules
spec:
  groups:
  # GRC Portal Application Alerts
  - name: grc-portal
    rules:
    - alert: GRCPortalDown
      expr: up{job="grc-portal"} == 0
      for: 1m
      labels:
        severity: critical
        component: application
      annotations:
        summary: "GRC Portal is down"
        description: "GRC Portal instance {{ $labels.instance }} has been down for more than 1 minute"

    - alert: GRCPortalHighMemory
      expr: |
        (container_memory_usage_bytes{container="grc-portal", namespace="shahin-grc"}
        / container_spec_memory_limit_bytes{container="grc-portal", namespace="shahin-grc"}) > 0.9
      for: 5m
      labels:
        severity: warning
        component: application
      annotations:
        summary: "GRC Portal high memory usage"
        description: "GRC Portal memory usage is above 90% for 5 minutes"

    - alert: GRCPortalHighCPU
      expr: |
        rate(container_cpu_usage_seconds_total{container="grc-portal", namespace="shahin-grc"}[5m]) > 1.8
      for: 5m
      labels:
        severity: warning
        component: application
      annotations:
        summary: "GRC Portal high CPU usage"
        description: "GRC Portal CPU usage is above 90% of limit for 5 minutes"

    - alert: GRCPortalPodRestarting
      expr: |
        increase(kube_pod_container_status_restarts_total{container="grc-portal", namespace="shahin-grc"}[1h]) > 3
      for: 0m
      labels:
        severity: warning
        component: application
      annotations:
        summary: "GRC Portal pod restarting frequently"
        description: "Pod {{ $labels.pod }} has restarted {{ $value }} times in the last hour"

    - alert: GRCPortalReplicasUnavailable
      expr: |
        kube_deployment_status_replicas_available{deployment="grc-portal", namespace="shahin-grc"}
        < kube_deployment_spec_replicas{deployment="grc-portal", namespace="shahin-grc"}
      for: 5m
      labels:
        severity: critical
        component: application
      annotations:
        summary: "GRC Portal replicas unavailable"
        description: "Only {{ $value }} replicas available, expected {{ $labels.spec_replicas }}"

  # PostgreSQL Database Alerts
  - name: postgresql
    rules:
    - alert: PostgreSQLDown
      expr: up{job="postgresql"} == 0
      for: 1m
      labels:
        severity: critical
        component: database
      annotations:
        summary: "PostgreSQL is down"
        description: "PostgreSQL instance {{ $labels.instance }} is down"

    - alert: PostgreSQLNoLeader
      expr: |
        count(patroni_master{namespace="shahin-grc"} == 1) != 1
      for: 1m
      labels:
        severity: critical
        component: database
      annotations:
        summary: "PostgreSQL cluster has no leader"
        description: "Patroni cluster has no elected leader. Automatic failover may have failed."

    - alert: PostgreSQLReplicaLagging
      expr: |
        pg_replication_lag{namespace="shahin-grc"} > 30
      for: 5m
      labels:
        severity: warning
        component: database
      annotations:
        summary: "PostgreSQL replication lag"
        description: "PostgreSQL replication lag is {{ $value }} seconds on {{ $labels.instance }}"

    - alert: PostgreSQLReplicaLagCritical
      expr: |
        pg_replication_lag{namespace="shahin-grc"} > 120
      for: 2m
      labels:
        severity: critical
        component: database
      annotations:
        summary: "PostgreSQL critical replication lag"
        description: "PostgreSQL replication lag is {{ $value }} seconds - data loss risk"

    - alert: PostgreSQLTooManyConnections
      expr: |
        sum(pg_stat_activity_count{namespace="shahin-grc"}) > 180
      for: 5m
      labels:
        severity: warning
        component: database
      annotations:
        summary: "PostgreSQL too many connections"
        description: "PostgreSQL has {{ $value }} connections (max 200)"

    - alert: PostgreSQLHighDiskUsage
      expr: |
        (pg_database_size_bytes{namespace="shahin-grc"}
        / (50 * 1024 * 1024 * 1024)) > 0.85
      for: 30m
      labels:
        severity: warning
        component: database
      annotations:
        summary: "PostgreSQL high disk usage"
        description: "PostgreSQL database {{ $labels.datname }} is using {{ $value | humanizePercentage }} of allocated storage"

  # etcd Cluster Alerts
  - name: etcd
    rules:
    - alert: EtcdClusterUnavailable
      expr: |
        count(etcd_server_has_leader{namespace="shahin-grc"} == 1) < 2
      for: 1m
      labels:
        severity: critical
        component: consensus
      annotations:
        summary: "etcd cluster unavailable"
        description: "etcd cluster has less than 2 members with a leader. Patroni failover will not work."

    - alert: EtcdMemberDown
      expr: |
        count(up{job="etcd", namespace="shahin-grc"} == 1) < 3
      for: 5m
      labels:
        severity: warning
        component: consensus
      annotations:
        summary: "etcd member down"
        description: "Only {{ $value }} etcd members are healthy. Quorum at risk."

    - alert: EtcdHighLatency
      expr: |
        histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket{namespace="shahin-grc"}[5m])) > 0.5
      for: 5m
      labels:
        severity: warning
        component: consensus
      annotations:
        summary: "etcd high fsync latency"
        description: "etcd fsync p99 latency is {{ $value }}s - may affect cluster performance"

    - alert: EtcdHighCommitDuration
      expr: |
        histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket{namespace="shahin-grc"}[5m])) > 0.25
      for: 5m
      labels:
        severity: warning
        component: consensus
      annotations:
        summary: "etcd high commit duration"
        description: "etcd backend commit p99 duration is {{ $value }}s"

  # Redis Alerts
  - name: redis
    rules:
    - alert: RedisDown
      expr: up{job="redis", namespace="shahin-grc"} == 0
      for: 1m
      labels:
        severity: critical
        component: cache
      annotations:
        summary: "Redis is down"
        description: "Redis instance {{ $labels.instance }} is down"

    - alert: RedisSentinelDown
      expr: |
        count(up{job="redis-sentinel", namespace="shahin-grc"} == 1) < 2
      for: 2m
      labels:
        severity: critical
        component: cache
      annotations:
        summary: "Redis Sentinel quorum lost"
        description: "Less than 2 Sentinel instances are healthy. Automatic failover disabled."

    - alert: RedisNoMaster
      expr: |
        count(redis_instance_info{role="master", namespace="shahin-grc"}) != 1
      for: 1m
      labels:
        severity: critical
        component: cache
      annotations:
        summary: "Redis has no master"
        description: "Redis cluster has no master node. Cache unavailable."

    - alert: RedisHighMemory
      expr: |
        redis_memory_used_bytes{namespace="shahin-grc"}
        / redis_memory_max_bytes{namespace="shahin-grc"} > 0.9
      for: 5m
      labels:
        severity: warning
        component: cache
      annotations:
        summary: "Redis high memory usage"
        description: "Redis memory usage is above 90% - eviction will occur"

    - alert: RedisTooManyConnections
      expr: redis_connected_clients{namespace="shahin-grc"} > 100
      for: 5m
      labels:
        severity: warning
        component: cache
      annotations:
        summary: "Redis too many connections"
        description: "Redis has {{ $value }} connected clients"

  # Kafka Alerts
  - name: kafka
    rules:
    - alert: KafkaBrokerDown
      expr: |
        count(up{job="kafka", namespace="shahin-grc"} == 1) < 3
      for: 2m
      labels:
        severity: critical
        component: messaging
      annotations:
        summary: "Kafka broker down"
        description: "Only {{ $value }} Kafka brokers are available (expected 3)"

    - alert: KafkaNoController
      expr: |
        count(kafka_controller_active_controller_count{namespace="shahin-grc"} == 1) != 1
      for: 1m
      labels:
        severity: critical
        component: messaging
      annotations:
        summary: "Kafka has no active controller"
        description: "Kafka cluster has no active controller. Cluster is unhealthy."

    - alert: KafkaUnderReplicatedPartitions
      expr: |
        kafka_server_replicamanager_underreplicatedpartitions{namespace="shahin-grc"} > 0
      for: 5m
      labels:
        severity: warning
        component: messaging
      annotations:
        summary: "Kafka under-replicated partitions"
        description: "{{ $value }} partitions are under-replicated on broker {{ $labels.instance }}"

    - alert: KafkaOfflinePartitions
      expr: |
        kafka_controller_offlinepartitionscount{namespace="shahin-grc"} > 0
      for: 1m
      labels:
        severity: critical
        component: messaging
      annotations:
        summary: "Kafka offline partitions"
        description: "{{ $value }} partitions are offline. Data may be unavailable."

    - alert: KafkaConsumerLag
      expr: |
        kafka_consumergroup_lag{namespace="shahin-grc", group="grc-consumer-group"} > 10000
      for: 10m
      labels:
        severity: warning
        component: messaging
      annotations:
        summary: "Kafka consumer lag"
        description: "Consumer group {{ $labels.group }} has lag of {{ $value }} messages"

  # PgBouncer/HAProxy Alerts
  - name: load-balancing
    rules:
    - alert: PgBouncerDown
      expr: up{job="pgbouncer", namespace="shahin-grc"} == 0
      for: 1m
      labels:
        severity: critical
        component: database
      annotations:
        summary: "PgBouncer is down"
        description: "PgBouncer connection pooler is down. Database connections will fail."

    - alert: HAProxyDown
      expr: up{job="haproxy", namespace="shahin-grc"} == 0
      for: 1m
      labels:
        severity: critical
        component: database
      annotations:
        summary: "HAProxy is down"
        description: "HAProxy database load balancer is down. Database routing unavailable."

    - alert: HAProxyBackendDown
      expr: |
        haproxy_backend_up{namespace="shahin-grc"} == 0
      for: 1m
      labels:
        severity: critical
        component: database
      annotations:
        summary: "HAProxy backend down"
        description: "HAProxy backend {{ $labels.backend }} has no healthy servers"
---
# Service Monitors for metric scraping
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: grc-portal-monitor
  namespace: shahin-grc
  labels:
    app.kubernetes.io/name: grc-portal
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: grc-portal
  namespaceSelector:
    matchNames:
    - shahin-grc
  endpoints:
  - port: http
    path: /metrics
    interval: 30s
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: postgres-monitor
  namespace: shahin-grc
  labels:
    app.kubernetes.io/name: postgresql
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: postgresql
  namespaceSelector:
    matchNames:
    - shahin-grc
  endpoints:
  - port: patroni
    path: /metrics
    interval: 30s
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: etcd-monitor
  namespace: shahin-grc
  labels:
    app.kubernetes.io/name: etcd
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: etcd
  namespaceSelector:
    matchNames:
    - shahin-grc
  endpoints:
  - port: client
    path: /metrics
    interval: 30s
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: redis-monitor
  namespace: shahin-grc
  labels:
    app.kubernetes.io/name: redis
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: redis
  namespaceSelector:
    matchNames:
    - shahin-grc
  endpoints:
  - port: redis
    interval: 30s
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: kafka-monitor
  namespace: shahin-grc
  labels:
    app.kubernetes.io/name: kafka
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: kafka
  namespaceSelector:
    matchNames:
    - shahin-grc
  endpoints:
  - port: client
    interval: 30s
